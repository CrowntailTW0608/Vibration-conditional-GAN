{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e84d2-8209-40b9-ac6d-2d03b7651639",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'py37_tf23' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n py37_tf23 ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class VibrationDataset:\n",
    "    def __init__(self, data_dir, condition_dim=5, batch_size=32, shuffle=True, validation_split=0.1, test_split=0.1):\n",
    "        self.data_dir = data_dir\n",
    "        self.condition_dim = condition_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.validation_split = validation_split\n",
    "        self.test_split = test_split\n",
    "        self.categories = sorted(os.listdir(self.data_dir))\n",
    "        self.num_categories = len(self.categories)\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        df = pd.read_csv(file_path)  # Assuming CSV format, adjust accordingly if using different file types\n",
    "        data = df.values\n",
    "        features = data[:, :-self.condition_dim]\n",
    "        conditions = data[:, -self.condition_dim:]\n",
    "        return features, conditions\n",
    "\n",
    "    def _load_category_data(self, category):\n",
    "        category_dir = os.path.join(self.data_dir, category)\n",
    "        file_names = sorted(os.listdir(category_dir))\n",
    "        file_paths = [os.path.join(category_dir, file_name) for file_name in file_names]\n",
    "\n",
    "        all_features = []\n",
    "        all_conditions = []\n",
    "        for file_path in file_paths:\n",
    "            features, conditions = self._load_data(file_path)\n",
    "            all_features.append(features)\n",
    "            all_conditions.append(conditions)\n",
    "\n",
    "        features = np.concatenate(all_features, axis=0)\n",
    "        conditions = np.concatenate(all_conditions, axis=0)\n",
    "\n",
    "        # Perform feature scaling (optional, but can be beneficial for training)\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "\n",
    "        return features, conditions\n",
    "\n",
    "    def _load_all_data(self):\n",
    "        all_features = []\n",
    "        all_conditions = []\n",
    "        for category in self.categories:\n",
    "            features, conditions = self._load_category_data(category)\n",
    "            all_features.append(features)\n",
    "            all_conditions.append(conditions)\n",
    "\n",
    "        features = np.concatenate(all_features, axis=0)\n",
    "        conditions = np.concatenate(all_conditions, axis=0)\n",
    "\n",
    "        return features, conditions\n",
    "\n",
    "    def _preprocess_dataset(self, features, conditions):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((features, conditions))\n",
    "\n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(features))\n",
    "\n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_dataset_splits(self):\n",
    "        features, conditions = self._load_all_data()\n",
    "\n",
    "        # 將整體數據集拆分成訓練集、驗證集和測試集\n",
    "        x_train, x_test, y_train, y_test = train_test_split(features, conditions, test_size=self.test_split, random_state=42)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=self.validation_split, random_state=42)\n",
    "\n",
    "        # 對特徵進行標準化\n",
    "        scaler = StandardScaler()\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_val = scaler.transform(x_val)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        # 創建訓練集、驗證集和測試集的tf.data.Dataset\n",
    "        train_dataset = self._preprocess_dataset(x_train, y_train)\n",
    "        val_dataset = self._preprocess_dataset(x_val, y_val)\n",
    "        test_dataset = self._preprocess_dataset(x_test, y_test)\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c8561",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "condition_dim = 5\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "validation_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "# 建立VibrationDataset實例\n",
    "vibration_dataset = VibrationDataset(data_dir, condition_dim, batch_size, shuffle, validation_split, test_split)\n",
    "\n",
    "# 獲取訓練集、驗證集和測試集\n",
    "train_dataset, val_dataset, test_dataset = vibration_dataset.get_dataset_splits()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
